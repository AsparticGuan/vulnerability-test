import torch
from transformers import RobertaForSequenceClassification, RobertaConfig, RobertaTokenizer

config = RobertaConfig.from_pretrained("config.json")
model = RobertaForSequenceClassification(config)
model.load_state_dict(torch.load('models/roberta_sentiment_analysis/pytorch_model.bin'))

# 将模型设置为评估模式
model.eval()

# 输入文本
text = "def isolate_ipython0(func):\n    def my_func():\n        ip0 = get_ipython()\n        if ip0 is None:\n            return func()\n        # We have a real ipython running...\n        user_ns = ip0.user_ns\n        user_global_ns = ip0.user_global_ns\n\n        # Previously the isolation was attempted with a deep copy of the user\n        # dicts, but we found cases where this didn't work correctly. I'm not\n        # quite sure why, but basically it did damage the user namespace, such\n        # that later tests stopped working correctly.  Instead we use a simpler\n        # approach, just computing the list of added keys to the namespace and\n        # eliminating those afterwards.  Existing keys that may have been\n        # modified remain modified.  So far this has proven to be robust.\n\n        # Compute set of old local/global keys\n        old_locals = set(user_ns.keys())\n        old_globals = set(user_global_ns.keys())\n        try:\n            out = func()\n        finally:\n            # Find new keys, and if any, remove them\n            new_locals = set(user_ns.keys()) - old_locals\n            new_globals = set(user_global_ns.keys()) - old_globals\n            for k in new_locals:\n                del user_ns[k]\n            for k in new_globals:\n                del user_global_ns[k]\n        return out\n\n    my_func.__name__ = func.__name__\n    return my_func\n\n#-----------------------------------------------------------------------------\n# Tests\n#-----------------------------------------------------------------------------"
tokenizer = RobertaTokenizer.from_pretrained("BBPE_30k")

# 对文本进行tokenize和编码
inputs = tokenizer(text, max_length=512, padding='max_length', truncation=True, return_tensors='pt')

# 将编码后的输入传递给模型进行预测
with torch.no_grad():
    outputs = model(inputs['input_ids'])

print(outputs.logits)
# 获取预测的分类结果
predictions = torch.argmax(outputs.logits, dim=1)
print("分析结果:", predictions.item())
