from tqdm.auto import tqdm
from transformers import RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer
from transformers import AdamW
from torch.utils.data import DataLoader, Dataset
from pathlib import Path
import random
import numpy as np
import os
import logging
import torch
import argparse
import json

logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)
logger = logging.getLogger(__name__)


def mlm(tensor): # 传入的tensor应当是一个二维张量
    rand = torch.rand(tensor.shape)
    mask_arr = (rand < 0.15) * (tensor > 2) # 按位置乘
    selection = torch.flatten(mask_arr[0].nonzero()).tolist()
    tensor[0][selection] = 4
    return tensor

def data_to_encodings(tokenizer, args):
    input_ids = []
    mask = []
    labels = []
    lines = []
    with open('20200621_Python_function_docstring_datasets_train.json') as f:
        for line in f:
            # 解析每一行为JSON对象
            lines = json.loads(line)
            sample = tokenizer(lines['function'], max_length=512, padding='max_length', truncation=True, return_tensors='pt')
            labels.append([0] if lines['label'] == 'Incorrect' else [1])
            mask.append(sample.attention_mask)
            input_ids.append(mlm(sample.input_ids.detach().clone()))
    input_ids = torch.cat(input_ids)
    mask = torch.cat(mask)
    labels = torch.tensor(labels)
    print(labels.shape)


    encodings = {
        'input_ids' : input_ids,
        'mask' : mask,
        'labels' : labels       
    }

    return encodings


class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return self.encodings['input_ids'].shape[0]

    def __getitem__(self, i):
        # return dictionary of input_ids, attention_mask, and labels for index i
        return {key: tensor[i] for key, tensor in self.encodings.items()}

def set_seed(seed=42):
    random.seed(seed)
    os.environ['PYHTONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--epochs', type=int, default=30)
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--lr', type=float, default=1e-5)
    parser.add_argument('--data_path', type=str, default='./data')
    parser.add_argument('--save_path', type=str, default='./models')
    parser.add_argument('--model_name', type=str, default='roberta_sentiment_analysis')
    parser.add_argument('--tokenizer_name', type=str, default='BBPE_30k')
    parser.add_argument('--config_name', type=str, default='./config.json')
    parser.add_argument('--max_length', type=int, default=512)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--device', type=str, default='cuda')

    args = parser.parse_args()

    set_seed(args.seed)

    tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)
    encodings = data_to_encodings(tokenizer, args)
    dataset = TextDataset(encodings)
    loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)

    config = RobertaConfig.from_pretrained(args.config_name)
    model = RobertaForSequenceClassification(config)

    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    if torch.cuda.device_count() > 1:
        logger.warning("device: %s, n_gpu: %s", device, torch.cuda.device_count())
        model = torch.nn.DataParallel(model)
    print("Current GPU device: ", device)
    # and move our model over to the selected device
    model.to(device)

     # activate training mode
    model.train()
    # initialize optimizer
    optim = AdamW(model.parameters(), lr=args.lr, no_deprecation_warning=True)

    epochs = args.epochs
    for epoch in range(epochs):
        # setup loop with TQDM and dataloader
        loop = tqdm(loader, leave=True) # leave：完成后是否保留进度条
        for batch in loop:
            # initialize calculated gradients (from prev step)
            optim.zero_grad()
            # pull all tensor batches required for training
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['mask'].to(device)
            labels = batch['labels'].to(device)
            # process
            outputs = model(input_ids, attention_mask=attention_mask,
                            labels=labels)
            # extract loss
            loss = outputs.loss
            # calculate loss for every parameter that needs grad update
            loss.backward()
            # update parameters
            optim.step()
            # print relevant info to progress bar
            loop.set_description(f'Epoch {epoch}')
            loop.set_postfix(loss=loss.item())
    
    model_to_save = model.module if hasattr(model, 'module') else model
    save_path = os.path.join(args.save_path, args.model_name)
    os.makedirs(save_path, exist_ok=True)
    output_model_file = os.path.join(save_path, "pytorch_model.bin")
    torch.save(model_to_save.state_dict(), output_model_file)
    torch.cuda.memory.empty_cache()


if __name__ == '__main__':
    main()
